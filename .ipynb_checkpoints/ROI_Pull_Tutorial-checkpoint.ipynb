{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ROI pull\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Data  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "## Step 1:\n",
    "\n",
    "# set data variables\n",
    "\n",
    "print(\"[INFO] PROGRAM STARTING....\")\n",
    "# path that holds the subjects\n",
    "data_path='/projects/niblab/experiments/project_milkshake/derivatives'\n",
    "\n",
    "# get subject task functional images (niftis)\n",
    "# -- note: this code grabs functional images,  usually resting or functional-tasks.\n",
    "# This code may be unique, may be more efficient way to grab user code.\n",
    "func_folders=glob.glob(os.path.join(data_path,'sub-*/*'))\n",
    "# sort images\n",
    "func_folders.sort()\n",
    "#print(func_imgs)\n",
    "\n",
    "\n",
    "# initialize fmri timeseries class object --inherites functional image list\n",
    "obj1 = FMRITimeseries(func_folders)\n",
    "# --test class\n",
    "#print(obj1.data_dict)\n",
    "\n",
    "# setup data dictionary with unique subject and task keys\n",
    "data_dict=obj1.setup_dictionary(filtered_func=True)\n",
    "#print(data_dict)\n",
    "\n",
    "\n",
    "# set a list of subject ids from the dictionary 1st level keys\n",
    "subject_ids=list(data_dict.keys())\n",
    "subject_ids.sort()\n",
    "subject_ct=len(subject_ids) # get count of subject dataset\n",
    "print(\"[INFO] Dictionary made, {} keys \\n[INFO] Keys: {}\".format(len(data_dict.keys()), subject_ids))\n",
    "\n",
    "\n",
    "# build chunklist\n",
    "chunk_list=obj1.build_chunklist(subject_ids=subject_ids)\n",
    "#print('[INFO] CPU ct: %s'%mp.cpu_count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Transform in asymmetrical space with `flirt`    \n",
    "**`flirt`**:  used to apply a saved transformation to a volume (`--applyxfm`, `-init`, `-out`)\n",
    "-- note for this the reference volume must be specified, it sets the voxel and image dimensions of the resulting volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#########################################\n",
    "## Step 2: Transform Functionals to match the atlas masks with flirt and fslmaths.\n",
    "run_flirt=False\n",
    "if run_flirt==True:\n",
    "    print('[INFO] transforming functionals to match the mask with flirt....')\n",
    "\n",
    "    flirt_start_time = time.time()\n",
    "    for chunk in chunk_list:\n",
    "        with mp.Pool(12) as p:\n",
    "            #print(chunk)\n",
    "            p.map(subject_loop, chunk)\n",
    "    print('[INFO] transformation process complete.')\n",
    "    flirt_process_time=time.time() - flirt_start_time\n",
    "    print(\"--- %s seconds ---\"%flirt_process_time )\n",
    "\n",
    "\n",
    "run_fslmaths=True\n",
    "if run_fslmaths==True:\n",
    "    print('[INFO] apply new threhold with fslmaths,threshold 0.9...')\n",
    "\n",
    "    fslmaths_start_time = time.time()\n",
    "    for chunk in chunk_list:\n",
    "        with mp.Pool(4) as p:\n",
    "            #print(chunk)\n",
    "            p.map(subject_loop, chunk)\n",
    "    fslmaths_process_time=time.time() - fslmaths_start_time\n",
    "    print(\"[INFO] process complete in --- %s seconds ---\"%fslmaths_process_time )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Threshold fMRI activations  `fslmaths -thr` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform_niftis(niftis, verbose=False, run_process=False):\n",
    "    reference_nifti='/projects/niblab/parcellations/chocolate_decoding_rois/mni2ace.nii.gz'\n",
    "    reference_mat='/projects/niblab/parcellations/chocolate_decoding_rois/mni2ace.mat'\n",
    "    for nii in niftis:\n",
    "\n",
    "        # setup and run flirt\n",
    "        nii=nii.replace('.nii.gz', '')\n",
    "        out=nii+'_3mm'\n",
    "        \n",
    "        # flirt command\n",
    "        flirt_cmd=\"flirt -in {} -ref {} -init {} -applyxfm -out {}\".format(nii, reference_nifti, reference_mat, out)\n",
    "        if verbose != False:\n",
    "            print('[INFO] flirt command: \\n{}'.format(flirt_cmd))\n",
    "        if run_process == True:\n",
    "            os.system(flirt_cmd)\n",
    "\n",
    "        # fslmaths command to threshold --use binary option for transforming masks\n",
    "        fslmaths_cmd='fslmaths {} -thr 0.9 {}'.format(out,out)\n",
    "        if verbose != False:\n",
    "            print('[INFO] fslmaths command: \\n{}'.format(fslmaths_cmd))\n",
    "        if run_process == True:\n",
    "            os.system(fslmaths_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate ROI regions for each individual with `fslmeants`\n",
    "`fslmeants: outputs the average of a timeseries.`  \n",
    "Used fslmeants to calculate volumes for that cluster on each individual, \n",
    "extracting ROI regions, by subject by condition. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pull_timeseries(file_list, bb300_path='/projects/niblab/parcellations/bigbrain300',roi_df='/projects/niblab/parcellations/bigbrain300/renaming.csv'):\n",
    "\n",
    "    \n",
    "    bad_subs=[]\n",
    "    #ICD.display(roi_df)\n",
    "\n",
    "    # load asymmetrical nifti roi files\n",
    "    asym_niftis=glob.glob(\"/projects/niblab/parcellations/bigbrain300/MNI152Asymmetrical_3mm/*.nii.gz\")\n",
    "\n",
    "    # load roi list\n",
    "    out_dir = os.path.join(data_path, 'rois/bigbrain300/funcs_uc')\n",
    "    #print('[INFO] output folder: \\t%s \\n'%out_dir)\n",
    "\n",
    "\n",
    "    # loop through the roi file list\n",
    "    #print(roi_list[:3])\n",
    "    for nifti in sorted(file_list):\n",
    "\n",
    "        subj_id = nifti.split(\"/\")[-1].split(\"_\")[0]\n",
    "        task_id = nifti.split(\"/\")[-1].split(\"_\")[2]\n",
    "        #print('[INFO] roi: %s %s \\n%s'%(subj_id, task_id, nifti))\n",
    "\n",
    "        # loop through roi reference list\n",
    "        for ref_nifti in sorted(asym_niftis):\n",
    "            #print('[INFO] reference roi: %s'%ref_nifti)\n",
    "            roi = ref_nifti.split('/')[-1].split(\".\")[0]\n",
    "            out_path = os.path.join(out_dir, \"{}_{}_{}_{}.txt\".format(subj_id, \"ses-1\", task_id, roi))\n",
    "            #print(roi, out_path)\n",
    "            cmd='fslmeants -i {} -o {} -m {}'.format(nifti, out_path, ref_nifti)\n",
    "            try:\n",
    "                #cmd='fslmeants -i {} -o {} -m {}'.format(nifti, out_path, ref_nifti)\n",
    "                print(\"Running shell command: {}\".format(cmd))\n",
    "                #os.system(cmd)\n",
    "                pass\n",
    "            except:\n",
    "                bad_subs.append((subj_id, task_id))\n",
    "        \n",
    "        #print('[INFO] finished processing for %s'%subj_id)\n",
    "        \n",
    "\n",
    "    return \"%s\"%bad_subs\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Combine each ROI clusters into a matrix, by subject per condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_subjects=[]\n",
    "def timeseries_concat(subject_id, task,verbose=True, run_process=True):\n",
    "    \n",
    "    #for subject_id in subject_ids:\n",
    "        \n",
    "        #tasks=list(data_dict[subject_id].keys())\n",
    "        \n",
    "        #for task in tasks:\n",
    "            \n",
    "\n",
    "        #print(subject_id, task, stim)\n",
    "        #print(os.path.join(beta_path, 'rois/big300/%s'%stim))\n",
    "        # get roi texts for subject / condition\n",
    "        roi_files = glob.glob(os.path.join(data_path, 'bigbrain300/individual_rois/funcs_uc/%s*%s*.txt'%(subject_id,task)))\n",
    "\n",
    "        df_lst=[]\n",
    "        #print(roi_files)\n",
    "\n",
    "\n",
    "        try:\n",
    "            for txt in roi_files: \n",
    "                #print(txt)\n",
    "                df_temp = pd.read_csv(txt, sep=\"\\n\", header=None)\n",
    "                #print(df_temp)\n",
    "                df_lst.append(df_temp)\n",
    "            #print(subject_id, task, len(df_lst))\n",
    "\n",
    "            df_concat= pd.concat(df_lst, axis=1, sort=False)\n",
    "            #print(df_concat)\n",
    "\n",
    "            # write output file \n",
    "\n",
    "            if not os.path.exists(os.path.join(data_path,'bigbrain300/roi_matrices/funcs_uc')):\n",
    "                if verbose==True:\n",
    "                    print('[INFO] making ',os.path.join(data_path,'bigbrain300/roi_matrices/funcs_uc'))\n",
    "\n",
    "                #os.makedirs(os.path.join(beta_path,'subject_matrices/%s'%stim))\n",
    "            outfile=os.path.join(data_path, 'bigbrain300/roi_matrices/funcs_uc/%s_ses-1_%s.txt'%(subject_id,task))\n",
    "            if verbose==True:\n",
    "                print('[PROCESSING] making file %s....'%outfile)\n",
    "            if run_process ==True:\n",
    "                df_concat.to_csv(outfile, header=None, index=None, sep='\\t')\n",
    "        except:\n",
    "            pass\n",
    "            error_subjects.append((subject_id,task))\n",
    "\n",
    "        #if error_subjects: print(error_subjects)\n",
    "        return error_subjects;\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build functional connectivity subject/condition matrices from the ROI subject/condition matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "# Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "   \n",
    "def make_subject_fcm(subj_id,task, save_files=True, heatmap=False, connectome=False,verbose=False):\n",
    "    \n",
    "    if subj_id not in fc_corr_dict:\n",
    "        fc_corr_dict[subj_id] = {}\n",
    "\n",
    "    if subj_id not in subj_list:\n",
    "        subj_list.append(subj_id)\n",
    "            \n",
    "    roi_matrices =glob.glob(os.path.join(data_path, 'bigbrain300/roi_matrices/funcs_uc/%s*%s*.txt'%(subj_id,task)))\n",
    "    #print(roi_matrices)\n",
    "    \n",
    "    \n",
    "    for matrix in roi_matrices:\n",
    "        \n",
    "        filename = matrix.split(\"/\")[-1] #\"%s_ses-1_%s.txt\"%(subj_id, task_id)\n",
    "        #print(\"\\n[INFO] Subject Matrix: %s \"%filename)\n",
    "        \n",
    "        title_str=filename.split(\".\")[0]\n",
    "\n",
    "        try:\n",
    "            # we load the text file timeseries into an array \n",
    "            np_arr = np.loadtxt(matrix)\n",
    "            #print(np_arr)\n",
    "\n",
    "\n",
    "            # call fit_transform from ConnectivityMeasure object\n",
    "            correlation_matrix = connectome_measure.fit_transform([np_arr])#[0]\n",
    "            correlations.append(correlation_matrix)\n",
    "            #print('[INFO] CORRELATION: ', correlation_matrix.shape)\n",
    "            print('[INFO] %s \\tTask Mean: %s \\tTask Std: %s'%(subj_id, round(np.mean(correlation_matrix),2), \n",
    "                                    round(np.std(correlation_matrix),2)))\n",
    "            \n",
    "            #np.fill_diagonal(correlation_matrix, 0)\n",
    "            \n",
    "            # Make outfile path for subject/condition fcms \n",
    "            outfile=os.path.join(data_path, 'bigbrain300/fc_data/matrices/%s'%filename)\n",
    "            #print(\"[INFO] Outfile: {}\".format(outfile))\n",
    "\n",
    "            if heatmap==True:\n",
    "                # plot subject correlation matrix\n",
    "                # Mask out the major diagonal\n",
    "\n",
    "                plotting.plot_matrix(correlation_matrix, cmap=cmap, colorbar=True,title=title_str, tri='lower')\n",
    "            \n",
    "            if connectome==True:\n",
    "                # We threshold to keep only the 20% of edges with the highest value\n",
    "                # because the graph is very dense\n",
    "                plotting.plot_connectome(correlation_matrix, coords_list, edge_threshold=\"90%\", title=title_str)\n",
    "\n",
    "            # do I save the files?\n",
    "            if save_files==True:\n",
    "                #print(\"[INFO] Outfile: {}\".format(outfile))\n",
    "\n",
    "                # save correlation as textfile\n",
    "                np.savetxt(outfile, correlation_matrix.transpose(2,0,1).reshape(3,-1))\n",
    "                \n",
    "                if verbose==True:\n",
    "                    print(\"[INFO] Outfile: {}\".format(outfile))\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            bad_subjects.append(subj_id)\n",
    "            pass\n",
    "    #print(\"[INFO] completed process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build functional connectivity subject matrices from averages of the conditions by subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through dictionary and make matrice for subject with 300 roi timeseries files\n",
    "concat_sub_timeseries=False\n",
    "make_fcms=True\n",
    "fc_corr_dict = {}\n",
    "\n",
    "for subject in sorted(list(data_dict.keys())):\n",
    "    \n",
    "    subj_list=[]\n",
    "    bad_subjects=[]\n",
    "    correlations = []\n",
    "    n_regions_extracted=300\n",
    "\n",
    "    for task in sorted(list(data_dict[subject].keys())):\n",
    "        if data_dict[subject][task]['bb300'] == 300:\n",
    "            #print(\"[INFO] \",task)\n",
    "\n",
    "            # concat subjects\n",
    "            if concat_sub_timeseries==True:\n",
    "                timeseries_concat(subject, task)\n",
    "                \n",
    "            # make fcms\n",
    "            if make_fcms==True:\n",
    "                cmap=\"Spectral\"\n",
    "                #timeseries_files=glob.glob('/projects/niblab/experiments/chocolate_milkshake/data/bigbrain300/roi_matrices/funcs_uc/sub-*_*.txt')\n",
    "                #print(len(timeseries_files))\n",
    "                # Initializing ConnectivityMeasure object with kind='correlation'\n",
    "                connectome_measure = ConnectivityMeasure(kind='correlation')#, vectorize=True)\n",
    "                \n",
    "                # make subject & condition/task connectivtiy matrix\n",
    "                make_subject_fcm(subject,task)\n",
    "                \n",
    "    if len(correlations) >= 2:\n",
    "        task1=list(data_dict[subject].keys())[0].split(\"-\")[1]\n",
    "        task2=list(data_dict[subject].keys())[1].split(\"-\")[1]\n",
    "\n",
    "        #print(\"[INFO] making mean FCM for subject\")                \n",
    "\n",
    "        # make large subject matrix from average of the condition matrices\n",
    "        mean_matrix = np.mean(correlations, axis=0).reshape(n_regions_extracted, n_regions_extracted)\n",
    "        #print(\"[INFO] MEAN FCM: \", mean_correlations.shape)\n",
    "\n",
    "        #print('[INFO] correlations: \\n', mean_matrix)\n",
    "        ## plotting\n",
    "        #print('[INFO] Plot of the mean functional connectivity matrix: \\n')\n",
    "        title = '%s correlation between %d regions, conditions %s and %s'%(subject, 300, task1, task2)\n",
    "        \n",
    "        out_img=os.path.join(data_path, \"bigbrain300/fc_data/heatmaps/%s_ses-1_task-%s-%s_heatmap.png\"%(subject,task1, task2))\n",
    "        out_avg_file=os.path.join(data_path, \"bigbrain300/fc_data/matrices/%s_ses-1_task-%s-%s_avg.txt\"%(subject, task1,task2))\n",
    "\n",
    "        \n",
    "        # save correlation as textfile\n",
    "        np.savetxt(out_avg_file, mean_matrix)#.transpose(2,0,1).reshape(3,-1))\n",
    "        \n",
    "        #print(\"[INFO] Outfile: {}\".format(out_avg_file))\n",
    "\n",
    "        # First plot the matrix\n",
    "        print('\\n[INFO] %s'%title)\n",
    "        display1 = plotting.plot_matrix(mean_matrix,figure=(9, 7), vmax=.8, vmin=-.8,\n",
    "                                      title=title, colorbar=True,\n",
    "                                       cmap='Spectral', tri='lower')\n",
    "        \n",
    "        \n",
    "        # We threshold to keep only the 20% of edges with the highest value\n",
    "        # because the graph is very dense\n",
    "        #display2 = plotting.plot_connectome(mean_correlations, coords_list,\n",
    "                #edge_threshold=\"90%\", title=title)\n",
    "\n",
    "\n",
    "        #display1.figure.savefig(out_img)\n",
    "        print('[INFO] Mean: %s \\tStd: %s'%(round(np.mean(mean_matrix),2), \n",
    "                                    round(np.std(mean_matrix),2)))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
